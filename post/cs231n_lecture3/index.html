<!doctype html><html lang=en dir=auto>
<head><meta charset=utf-8>
<meta http-equiv=x-ua-compatible content="IE=edge">
<meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no">
<meta name=robots content="index, follow">
<title>[CS231n] Lecture 3: Loss Functions and Optimization 강의노트 | 잡다한 코딩스터디 WeCoding</title>
<meta name=keywords content="CS231n,deep-learning,Computer-Vision">
<meta name=description content="Stanford University CS231n 이론 스터디">
<meta name=author content="은비">
<link rel=canonical href=https://wecoding20.git.io/post/cs231n_lecture3/>
<meta name=google-site-verification content="XYZabc">
<meta name=yandex-verification content="XYZabc">
<meta name=msvalidate.01 content="XYZabc">
<link crossorigin=anonymous href=/assets/css/stylesheet.min.e75ceec3845a641a06716e240e4ae8e8b9bfec458754fa27c06dbdbac99b79c7.css integrity="sha256-51zuw4RaZBoGcW4kDkro6Lm/7EWHVPonwG29usmbecc=" rel="preload stylesheet" as=style>
<script defer crossorigin=anonymous src=/assets/js/highlight.min.7680afc38aa6b15ddf158a4f3780b7b1f7dde7e91d26f073e6229bb7a0793c92.js integrity="sha256-doCvw4qmsV3fFYpPN4C3sffd5+kdJvBz5iKbt6B5PJI=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=16x16 href=https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E>
<link rel=icon type=image/png sizes=32x32 href=https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E>
<link rel=apple-touch-icon href=https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E>
<link rel=mask-icon href=https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E>
<meta name=theme-color content="#2e2e33">
<meta name=msapplication-TileColor content="#2e2e33">
<meta name=generator content="Hugo 0.87.0">
<script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)}(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-123-45','auto'),ga('send','pageview'))</script><meta property="og:title" content="[CS231n] Lecture 3: Loss Functions and Optimization 강의노트">
<meta property="og:description" content="Stanford University CS231n 이론 스터디">
<meta property="og:type" content="article">
<meta property="og:url" content="https://wecoding20.git.io/post/cs231n_lecture3/"><meta property="og:image" content="https://wecoding20.git.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="post">
<meta property="article:published_time" content="2021-08-21T00:00:00+00:00">
<meta property="article:modified_time" content="2021-08-21T00:00:00+00:00"><meta property="og:site_name" content="WeCoding 위코딩">
<meta name=twitter:card content="summary_large_image">
<meta name=twitter:image content="https://wecoding20.git.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E">
<meta name=twitter:title content="[CS231n] Lecture 3: Loss Functions and Optimization 강의노트">
<meta name=twitter:description content="Stanford University CS231n 이론 스터디">
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://wecoding20.git.io/post/"},{"@type":"ListItem","position":2,"name":"[CS231n] Lecture 3: Loss Functions and Optimization 강의노트","item":"https://wecoding20.git.io/post/cs231n_lecture3/"}]}</script>
<script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"[CS231n] Lecture 3: Loss Functions and Optimization 강의노트","name":"[CS231n] Lecture 3: Loss Functions and Optimization 강의노트","description":"Stanford University CS231n 이론 스터디","keywords":["CS231n","deep-learning","Computer-Vision"],"articleBody":"CS231n Lecture3 유튜브 강의 링크\nCS231n Lecture3 pdf 자료\n 🚀 개요 Loss Fuction을 통한 w(가중치) 최적화   SVM Loss\n  Softmax\n  Optimization\n   Multi class SVM Loss w(가중치)가 좋은지 나쁜지 평가하기 위한 방법이 필요하다. score를 통해 w를 정량화하는 방법이 Loss Function를 통한 최적화이다.\n Loss Fuction (손실함수)  예측값과 실제값의 차이인 loss를 도출하는 함수다. loss fuction을 통해 w(가중치)가 얼마나 나쁜지 정량적으로 판단할 수 있다. loss가 낮을 수록 그 classifier의 성능이 좋다고 판단할 수 있다.\n Optimization (최적화)  loss fuction를 최소화할 수 있는 parameter를 찾는 방법을 의미한다. 어떤 w의 loss가 가장 적은지 판단하여 실제값과 loss의 차이를 줄일 수 있다.\nLoss Fuction의 계산 과정은 다음과 같다.\n  Sj = 오답 클래스의 예측 점수\n  Syi = 실제 정답 클래스의 예측 점수\n  SVM Loss는 스코어간의 차이에 주목한다. 뒤에 따라오는 1 값은 Safty margin으로, 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 최소한 얼마나 커야하는지 말해준다.\nSVM Loss 그래프는 경첩과 비슷하게 생겨서 Hinge loss 라고도 부른다. 어떤 카테고리에 대하여 Syi의 값이 (Sj+ 1)의 값보다 크면 loss는 0이며, 이미지를 잘 분류한다고 할 수 있다. 그러나 그게 아닐 경우 Syi의 값이 작아질 수록 loss 값이 커지는 모습을 확인할 수 있다.\nSVM Loss를 직접 계산해 본 결과는 다음과 같다.\nloss가 0에 가까울 수록 classifier의 성능이 좋다고 할 수 있다. 그러나 고양이, 차, 개구리를 분류하는 과정에서 loss의 평균값은 5.27이기 때문에 별로 좋은 score가 아니다.\nloss의 값이 0이 되도록 하는 w는 유일하지 않다. w가 2w, 3w, 4w가 되어도 loss의 값은 변함이 없다.\n고양이, 차, 개구리를 분류하는 과정에서 2w가 되어도 loss의 값은 0이 된다.\n이렇게 단순하게 loss의 값이 0이 되는 w를 찾는 것은 training data에만 맞는 loss fuction을 구하는 것과 같다. training set의 loss가 줄어들더라도 우리가 실제로 넣고자 하는 test set에서도 loss가 줄어들지는 알 수 없기 때문에 overfitting 문제가 발생할 수 있다.\n  파란 점 : training data\n  초록 점 : test data\n  training set의 loss 값을 0으로 만든 분류기는 파란색 선과 같다. 저 파란색 선은 test data가 들어왔을 때 overfitting 문제가 나타난다. 그래서 우리는 training data에도 잘 들어맞고, test data에도 잘 들어맞는 초록색 선과 같은 classifier를 지향해야한다.\n Overfitting (과적합)\n너무 과도하게 데이터에 대해 모델을 learning한 경우를 의미한다. 우리가 사실 원하는 정보는 기존에 알고 있는 데이터에 대한 것들이 아니라 새롭게 우리가 알게되는 데이터에 대한 것들을 알고 싶은 것인데, 정작 새로운 데이터에 대해서는 하나도 못맞추고, 즉 제대로 설명할 수 없는 경우라면 그 시스템은 그야말로 무용지물이라고 할 수 있을 것이다.\n출처 : Machine Learning 스터디 (3) Overfitting\n 우리는 단순하고 최대한 간결한 classifier를 통해 training set을 넣었을 때만큼 test set을 넣었을 때도 잘 동작할 수 있도록 결과를 도출해야 한다. 여기서 Regularization을 사용하여 loss function에 넣어준다.\nregularization은 예측모델(W)이 복잡한 고차 다항식을 선택할 때 페널티를 주어서, 단순한 저차 다항식을 선택하도록 만든다. 즉 training data에 완전히 fit하는 overfitting 문제가 나타나지 못하도록 페널티를 주는 것이다.\n λ : R(W)의 값을 맞춰주기 위한 hyperparameter이다. 너무 큰 값이면 지나치게 간결해져서 loss 값을 구하는 의미가 없어지고, 너무 작은 값이면 Regularization 의미가 없어지기 때문에 잘 설정해야한다.  Regularization에는 L2 regularization, L1 regularization, Dropout 등등이 있지만, 일반적으로 많이 사용되는 L2에 대해서 알아볼 예정이다.\nL2는 가중치 값이 비교적 균일하게 분포되어 있을 때 덜 복잡한 것으로 간주한다. 가중치 조합 w1= [0, 0, 0, 1] 과 w2= [0.25, 0.25, 0.25, 0.25]가 있다고 할 때, 균일하게 분포되어 있는 w2값을 선택한다. x의 모든 원소가 골고루 영향을 미치는 것을 선호하는 것이다.\n또다른 classifier인 Softmax Classifier은 SVM과는 다른 방법으로 loss를 계산한다.\nSVM Loss에서는 score 자체의 의미를 파악하기 보다는 단순하게 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 높기를 기대했다. 그러나 softmax classifier는 클래스 별로 확률 분포를 얻어 예측 점수 자체에 의미를 부여한다.\nloss 연산 과정은 다음과 같다.\n주어진 score에 대해 exp 연산을 거쳐서 점수를 반환한다. 산출된 값을 -log 연산을 취해 최종 확률 분포를 계산한다. 이때 exp연산을 거치는 계산이 softmax 연산 (multinomial logistic regression)에 해당한다.\nSVM과 softmax를 비교한 그림이다. SVM은 loss의 값이 0이 되는 순간, 더 이상 성능 개선이 이루어지지 않는다. 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 높으면 자기 할 일이 끝난 것이다. 그러나 softmax의 경우 최대한 확률을 높이기 위해 연산이 반복되며 성능이 향상된다. 따라서 웬만해서는 loss의 값이 0이 아니라 0에 근접한 값으로 나타난다.\nOptimization은 loss의 값을 최소화하는 최종 w를 찾아내기 위한 과정이다.\n고지가 높은 산에서 내려갈 때 우리는 반복적인 행동을 통하여 산을 내려간다. 산의 높이를 loss, 우리의 위치를 w라고 할 때 우리의 목표는 loss가 0인 산의 밑바닥에 도착하는 것이다. 우리는 loss가 적은 방향으로 w의 값을 변화시키면서 산을 내려가는데 Optimization을 통해 효율적으로 산을 내려가고자 할 것이다.\nOptimization의 가장 단순한 방법은 Random search로, 임의의 w를 계산하여 모든 loss를 비교하는 방식이다. CIFAR-10에서 test set을 넣어본 결과 15.5%의 처참한 정확도가 나온다. 참고로 2017년 당시 현대 최신 기술로는 95%의 정확도가 나온다고 한다.\n두 번째 방법은 Follow the slope 방법이다. 우리가 서있는 땅의 기울기를 구하여 어느 방향으로 갈 지를 결정하는 것을 말한다. 여기서 기울기는 어떤 함수에 대한 미분값을 의미한다. 단순해서 잘 동작하며 NN, Linear Classifier 학습에 많이 사용된다.\n다음은 Numerical gradient를 이용하여 기울기를 구하는 방법이다. w에 작은 값의 h를 더해 loss를 계산한다.\n그러나 이렇게 일일히 함수를 계산하는 것은 매우 비효율적이기 때문에, 우리는 Analytic gradient를 통해 식 하나로 기울기를 계산한다.\nAnalytic gradient에서는 함수와 가중치 w만 있으면 기울기를 바로 구할 수 있다.\n즉, Numerical gradient는 느리고 정확한 값은 아니지만, 계산이 쉽다. 그러나 Analytic gradient는 빠르고 정확한 값을 얻을 수 있지만, 코드 작성이 복잡해서 에러가 발생하기 쉽다.\n정리하자면 우리는 기울기를 구하고, loss의 최소값을 구해야하기 때문에 기울기가 증가하는 반대 방향으로 움직이면서 loss의 최소값을 찾는다.\nGradient Descent를 통해 가장 적절한 가중치를 찾아낼 수 있다.\nevaluate_gradient을 통해 미분값을 계산하고 weights에서 (step_size * 기울기) 만큼 빼준다. 뺄셈을 하는 이유는 기울기가 증가하기 때문에 반대로 감소하도록 해서 조금씩 이동하여 최종 loss의 값을 얻고자 함이다. Step_size는 어느 정도의 크기로 이동할 것인지 보여주기 때문에 학습 속도에 큰 영향을 미치는 중요한 hyperparameter이다.\n다음 그림은 2차원 공간에서 loss function이 그릇 모양이라고 가정을 할 때다. 빨간색 영역은 loss의 값이 낮은 부분, 파란색 영역으로 갈 수록 loss의 값이 커진다. w의 위치가 초록색 영역에 있다면 w는 빨간색 영역으로 가기 위해 gradient를 반복하여 계산한다. 따라서 매 step 마다 w는 조금씩 -gradient 방향으로 이동하여 w의 위치를 업데이트한다. 이 과정을 update rule이라고 한다.\nGradient Descent를 계산할 때 데이터의 수가 클수록 시간이 오래 걸린다. Minibatch(데이터의 일부분)에 대한 기울기를 통해 실제 loss 값과 gradient를 추정하여 w를 업데이트하는 방식이다.\nx, y, Softmax/SVM, Regularization 등의 값을 조절할 수 있는 실습 사이트이다.\nInteractive Web Demo Site\n이미지를 분류하기 위해서는 이미지의 Feature(특징)을 추출해야 한다.\n이미지의 특징을 특징 벡터로 변환한다.\n왼쪽 그림에서 linear classifier로 결정계를 구분할 수 없기 때문에 Feature Transform(특징변환)을 하여 점들을 오른쪽 그림과 같이 선형 벡터로 변환한다. 오른쪽 그림에서 이제 선형함수로 분리할 수 있게 되었다.\n그렇다면 어떤 방식으로 이미지를 feature transform 할 수 있을까?\n  Color Histogram : 특정 색에 해당하는 픽셀의 수를 계산하여 히스토그램으로 표현\n  HIstogram of Oriented Gradients (Hog) : 이미지를 8x8 픽셀로 나누어 가장 지배적인 edge를 찾는다. Edge를 양자화시켜 edge orientation 히스토그램으로 표현한다.\n  Bag of Words : 이미지를 작게 잘라 시각 단어로 표현한 후,K-means 알고리즘으로 1천개의 중심점을 갖도록 학습을 한다. 이미지의 시각 단어의 발생 빈도를 인코딩하여 이미지의 특징을 추출한다.\n  2012년까지는 Image feature을 추출하는 것이 중요했지만, AlexNet이 나온 뒤로는 layer을 쌓으면서 데이터로부터 특징을 직접 추출하여 학습하는 방식을 사용한다. Linear classifier, 가중치를 한 번에 학습하기 때문에 CNN의 구조를 잘 짜는 것이 더욱 중요해졌다.\n  참고\n*[CS231n] Lecture 3 - Loss Suctions and Optimization*\n*[풀잎스쿨] CS231n 3강(1) Loss Functions*\n*[풀잎스쿨] CS231n 3강 (2) Optimization \u0026 Image Features*\n*[CS231n] 3. Loss Functions and Optimization*\n누구나 이해할 수 있는 딥러닝 - cs231n 3강(Loss Functions and Optimization)\n ","wordCount":"1154","inLanguage":"en","datePublished":"2021-08-21T00:00:00Z","dateModified":"2021-08-21T00:00:00Z","author":{"@type":"Person","name":"은비"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://wecoding20.git.io/post/cs231n_lecture3/"},"publisher":{"@type":"Organization","name":"잡다한 코딩스터디 WeCoding","logo":{"@type":"ImageObject","url":"https://wecoding20.git.io/%3Clink%20/%20abs%20url%3E"}}}</script>
</head>
<body id=top>
<script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add('dark'):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove('dark'):window.matchMedia('(prefers-color-scheme: dark)').matches&&document.body.classList.add('dark')</script>
<noscript>
<style type=text/css>#theme-toggle,.top-link{display:none}</style>
<style>@media(prefers-color-scheme:dark){:root{--theme:#1d1e20;--entry:#2e2e33;--primary:rgba(255, 255, 255, 0.84);--secondary:rgba(255, 255, 255, 0.56);--tertiary:rgba(255, 255, 255, 0.16);--content:rgba(255, 255, 255, 0.74);--hljs-bg:#2e2e33;--code-bg:#37383e;--border:#333}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style>
</noscript>
<header class=header>
<nav class=nav>
<div class=logo>
<a href=https://wecoding20.git.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches>
<button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg>
</button>
</span>
</div>
<ul id=menu>
<li>
<a href=https://wecoding20.git.io/archives title=Archive>
<span>Archive</span>
</a>
</li>
<li>
<a href=https://wecoding20.git.io/categories/ title=Categories>
<span>Categories</span>
</a>
</li>
<li>
<a href=https://wecoding20.git.io/profile/ title=Profile>
<span>Profile</span>
</a>
</li>
<li>
<a href=https://wecoding20.git.io/search/ title="Search (Alt + /)" accesskey=/>
<span>Search</span>
</a>
</li>
<li>
<a href=https://wecoding20.git.io/tags/ title=Tags>
<span>Tags</span>
</a>
</li>
</ul>
</nav>
</header>
<main class=main>
<article class=post-single>
<header class=post-header>
<div class=breadcrumbs><a href=https://wecoding20.git.io/>Home</a>&nbsp;»&nbsp;<a href=https://wecoding20.git.io/post/>Posts</a></div>
<h1 class=post-title>
[CS231n] Lecture 3: Loss Functions and Optimization 강의노트
</h1>
<div class=post-description>
Stanford University CS231n 이론 스터디
</div>
<div class=post-meta>August 21, 2021&nbsp;·&nbsp;6 min&nbsp;·&nbsp;은비&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/post/CS231n_lecture3.md rel="noopener noreferrer" target=_blank>Suggest Changes</a>
</div>
</header>
<div class=post-content><p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image1.png alt=CS231n_lecture3_image1>
</p>
<p><a href="https://youtu.be/h7iBpEHGVNc?list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk">CS231n Lecture3 유튜브 강의 링크</a><br>
<a href=http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture3.pdf>CS231n Lecture3 pdf 자료</a></p>
<hr>
<h2 id=-개요>🚀 개요<a hidden class=anchor aria-hidden=true href=#-개요>#</a></h2>
<h4 id=loss-fuction을-통한-w가중치-최적화><strong><strong>Loss Fuction을 통한 w(가중치) 최적화</strong></strong><a hidden class=anchor aria-hidden=true href=#loss-fuction을-통한-w가중치-최적화>#</a></h4>
<ul>
<li>
<p>SVM Loss</p>
</li>
<li>
<p>Softmax</p>
</li>
<li>
<p>Optimization</p>
</li>
</ul>
<hr>
<h2 id=multi-class-svm-loss>Multi class SVM Loss<a hidden class=anchor aria-hidden=true href=#multi-class-svm-loss>#</a></h2>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image2.png alt=CS231n_lecture3_image2>
<br>
w(가중치)가 좋은지 나쁜지 평가하기 위한 방법이 필요하다. score를 통해 w를 정량화하는 방법이 Loss Function를 통한 최적화이다.</p>
<ul>
<li><strong><strong>Loss Fuction (손실함수)</strong></strong></li>
</ul>
<p>예측값과 실제값의 차이인 loss를 도출하는 함수다. loss fuction을 통해 w(가중치)가 얼마나 나쁜지 정량적으로 판단할 수 있다. loss가 낮을 수록 그 classifier의 성능이 좋다고 판단할 수 있다.</p>
<ul>
<li><strong><strong>Optimization (최적화)</strong></strong></li>
</ul>
<p>loss fuction를 최소화할 수 있는 parameter를 찾는 방법을 의미한다. 어떤 w의 loss가 가장 적은지 판단하여 실제값과 loss의 차이를 줄일 수 있다.</p>
<p>Loss Fuction의 계산 과정은 다음과 같다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image3.png alt=CS231n_lecture3_image3>
</p>
<ul>
<li>
<p><strong><strong>Sj</strong></strong> = 오답 클래스의 예측 점수</p>
</li>
<li>
<p><strong><strong>Syi</strong></strong> = 실제 정답 클래스의 예측 점수</p>
</li>
</ul>
<p>SVM Loss는 스코어간의 차이에 주목한다. 뒤에 따라오는 <strong><strong>1</strong></strong> 값은 Safty margin으로, <strong><strong>정답 클래스의 예측 점수</strong></strong>가 <strong><strong>오답 클래스의 예측 점수</strong></strong>보다 최소한 얼마나 커야하는지 말해준다.</p>
<p>SVM Loss 그래프는 경첩과 비슷하게 생겨서 <strong><strong>Hinge loss</strong></strong> 라고도 부른다. 어떤 카테고리에 대하여 Syi의 값이 (Sj + 1)의 값보다 크면 loss는 0이며, 이미지를 잘 분류한다고 할 수 있다. 그러나 그게 아닐 경우 Syi의 값이 작아질 수록 loss 값이 커지는 모습을 확인할 수 있다.</p>
<p>SVM Loss를 직접 계산해 본 결과는 다음과 같다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image4.png alt=CS231n_lecture3_image4>
</p>
<p>loss가 0에 가까울 수록 classifier의 성능이 좋다고 할 수 있다. 그러나 고양이, 차, 개구리를 분류하는 과정에서 loss의 평균값은 5.27이기 때문에 별로 좋은 score가 아니다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image5.png alt=CS231n_lecture3_image5>
</p>
<p>loss의 값이 0이 되도록 하는 w는 유일하지 않다. w가 2w, 3w, 4w가 되어도 loss의 값은 변함이 없다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image6.png alt=CS231n_lecture3_image6>
</p>
<p>고양이, 차, 개구리를 분류하는 과정에서 2w가 되어도 loss의 값은 0이 된다.</p>
<p>이렇게 단순하게 loss의 값이 0이 되는 w를 찾는 것은 training data에만 맞는 loss fuction을 구하는 것과 같다. training set의 loss가 줄어들더라도 우리가 실제로 넣고자 하는 test set에서도 loss가 줄어들지는 알 수 없기 때문에 <strong><strong>overfitting</strong></strong> 문제가 발생할 수 있다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image7.png alt=CS231n_lecture3_image7>
</p>
<ul>
<li>
<p><strong><strong>파란 점</strong></strong> : training data</p>
</li>
<li>
<p><strong><strong>초록 점</strong></strong> : test data</p>
</li>
</ul>
<p>training set의 loss 값을 0으로 만든 분류기는 파란색 선과 같다. 저 파란색 선은 test data가 들어왔을 때 overfitting 문제가 나타난다. 그래서 우리는 training data에도 잘 들어맞고, test data에도 잘 들어맞는 초록색 선과 같은 classifier를 지향해야한다.</p>
<blockquote>
<p><em><strong><strong>Overfitting (과적합)</strong></strong></em></p>
<p><em>너무 과도하게 데이터에 대해 모델을 learning한 경우를 의미한다. 우리가 사실 원하는 정보는 기존에 알고 있는 데이터에 대한 것들이 아니라 새롭게 우리가 알게되는 데이터에 대한 것들을 알고 싶은 것인데, 정작 새로운 데이터에 대해서는 하나도 못맞추고, 즉 제대로 설명할 수 없는 경우라면 그 시스템은 그야말로 무용지물이라고 할 수 있을 것이다.</em></p>
<p><em>출처 :</em> <a href=http://sanghyukchun.github.io/59/><em>Machine Learning 스터디 (3) Overfitting</em></a></p>
</blockquote>
<p>우리는 단순하고 최대한 간결한 classifier를 통해 training set을 넣었을 때만큼 test set을 넣었을 때도 잘 동작할 수 있도록 결과를 도출해야 한다. 여기서 Regularization을 사용하여 loss function에 넣어준다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image8.png alt=CS231n_lecture3_image8>
</p>
<p>regularization은 예측모델(W)이 복잡한 고차 다항식을 선택할 때 페널티를 주어서, 단순한 저차 다항식을 선택하도록 만든다. 즉 training data에 완전히 fit하는 overfitting 문제가 나타나지 못하도록 페널티를 주는 것이다.</p>
<ul>
<li><strong><strong>λ</strong></strong> : R(W)의 값을 맞춰주기 위한 hyperparameter이다. 너무 큰 값이면 지나치게 간결해져서 loss 값을 구하는 의미가 없어지고, 너무 작은 값이면 Regularization 의미가 없어지기 때문에 잘 설정해야한다.</li>
</ul>
<p>Regularization에는 L2 regularization, L1 regularization, Dropout 등등이 있지만, 일반적으로 많이 사용되는 L2에 대해서 알아볼 예정이다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image9.png alt=CS231n_lecture3_image9>
</p>
<p>L2는 가중치 값이 비교적 균일하게 분포되어 있을 때 덜 복잡한 것으로 간주한다. 가중치 조합 w1 = [0, 0, 0, 1] 과 w2 = [0.25, 0.25, 0.25, 0.25]가 있다고 할 때, 균일하게 분포되어 있는 w2 값을 선택한다. x의 모든 원소가 골고루 영향을 미치는 것을 선호하는 것이다.</p>
<p>또다른 classifier인 Softmax Classifier은 SVM과는 다른 방법으로 loss를 계산한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image10.png alt=CS231n_lecture3_image10>
</p>
<p>SVM Loss에서는 score 자체의 의미를 파악하기 보다는 단순하게 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 높기를 기대했다. 그러나 softmax classifier는 클래스 별로 확률 분포를 얻어 예측 점수 자체에 의미를 부여한다.</p>
<p>loss 연산 과정은 다음과 같다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image11.png alt=CS231n_lecture3_image11>
</p>
<p>주어진 score에 대해 exp 연산을 거쳐서 점수를 반환한다. 산출된 값을 -log 연산을 취해 최종 확률 분포를 계산한다. 이때 exp연산을 거치는 계산이 softmax 연산 (multinomial logistic regression)에 해당한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image12.png alt=CS231n_lecture3_image12>
</p>
<p>SVM과 softmax를 비교한 그림이다. SVM은 loss의 값이 0이 되는 순간, 더 이상 성능 개선이 이루어지지 않는다. 정답 클래스의 예측 점수가 오답 클래스의 예측 점수보다 높으면 자기 할 일이 끝난 것이다. 그러나 softmax의 경우 최대한 확률을 높이기 위해 연산이 반복되며 성능이 향상된다. 따라서 웬만해서는 loss의 값이 0이 아니라 0에 근접한 값으로 나타난다.</p>
<p>Optimization은 loss의 값을 최소화하는 최종 w를 찾아내기 위한 과정이다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image13.png alt=CS231n_lecture3_image13>
</p>
<p>고지가 높은 산에서 내려갈 때 우리는 반복적인 행동을 통하여 산을 내려간다. 산의 높이를 loss, 우리의 위치를 w라고 할 때 우리의 목표는 loss가 0인 산의 밑바닥에 도착하는 것이다. 우리는 loss가 적은 방향으로 w의 값을 변화시키면서 산을 내려가는데 Optimization을 통해 효율적으로 산을 내려가고자 할 것이다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image14.png alt=CS231n_lecture3_image14>
</p>
<p>Optimization의 가장 단순한 방법은 <strong><strong>Random search</strong></strong>로, 임의의 w를 계산하여 모든 loss를 비교하는 방식이다. CIFAR-10에서 test set을 넣어본 결과 15.5%의 처참한 정확도가 나온다. 참고로 2017년 당시 현대 최신 기술로는 95%의 정확도가 나온다고 한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image15.png alt=CS231n_lecture3_image15>
</p>
<p>두 번째 방법은 <strong><strong>Follow the slope</strong></strong> 방법이다. 우리가 서있는 땅의 기울기를 구하여 어느 방향으로 갈 지를 결정하는 것을 말한다. 여기서 기울기는 어떤 함수에 대한 미분값을 의미한다. 단순해서 잘 동작하며 NN, Linear Classifier 학습에 많이 사용된다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image16.png alt=CS231n_lecture3_image16>
</p>
<p>다음은 <strong><strong>Numerical gradient</strong></strong>를 이용하여 기울기를 구하는 방법이다. w에 작은 값의 h를 더해 loss를 계산한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image17.png alt=CS231n_lecture3_image17>
</p>
<p>그러나 이렇게 일일히 함수를 계산하는 것은 매우 비효율적이기 때문에, 우리는 <strong><strong>Analytic gradient</strong></strong>를 통해 식 하나로 기울기를 계산한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image18.png alt=CS231n_lecture3_image18>
</p>
<p>Analytic gradient에서는 함수와 가중치 w만 있으면 기울기를 바로 구할 수 있다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image19.png alt=CS231n_lecture3_image19>
</p>
<p>즉, Numerical gradient는 느리고 정확한 값은 아니지만, 계산이 쉽다. 그러나 Analytic gradient는 빠르고 정확한 값을 얻을 수 있지만, 코드 작성이 복잡해서 에러가 발생하기 쉽다.</p>
<p>정리하자면 우리는 기울기를 구하고, loss의 최소값을 구해야하기 때문에 기울기가 증가하는 반대 방향으로 움직이면서 loss의 최소값을 찾는다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image20.png alt=CS231n_lecture3_image20>
</p>
<p><strong><strong>Gradient Descent</strong></strong>를 통해 가장 적절한 가중치를 찾아낼 수 있다.</p>
<p>evaluate_gradient을 통해 미분값을 계산하고 weights에서 (step_size * 기울기) 만큼 빼준다. 뺄셈을 하는 이유는 기울기가 증가하기 때문에 반대로 감소하도록 해서 조금씩 이동하여 최종 loss의 값을 얻고자 함이다. Step_size는 어느 정도의 크기로 이동할 것인지 보여주기 때문에 학습 속도에 큰 영향을 미치는 중요한 hyperparameter이다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image21.png alt=CS231n_lecture3_image21>
</p>
<p>다음 그림은 2차원 공간에서 loss function이 그릇 모양이라고 가정을 할 때다. 빨간색 영역은 loss의 값이 낮은 부분, 파란색 영역으로 갈 수록 loss의 값이 커진다. w의 위치가 초록색 영역에 있다면 w는 빨간색 영역으로 가기 위해 gradient를 반복하여 계산한다. 따라서 매 step 마다 w는 조금씩 -gradient 방향으로 이동하여 w의 위치를 업데이트한다. 이 과정을 <strong><strong>update rule</strong></strong>이라고 한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image22.png alt=CS231n_lecture3_image22>
</p>
<p>Gradient Descent를 계산할 때 데이터의 수가 클수록 시간이 오래 걸린다. Minibatch(데이터의 일부분)에 대한 기울기를 통해 실제 loss 값과 gradient를 추정하여 w를 업데이트하는 방식이다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image23.png alt=CS231n_lecture3_image23>
</p>
<p>x, y, Softmax/SVM, Regularization 등의 값을 조절할 수 있는 실습 사이트이다.</p>
<p><a href=http://vision.stanford.edu/teaching/cs231n-demos/linear-classify/>Interactive Web Demo Site</a></p>
<p>이미지를 분류하기 위해서는 이미지의 Feature(특징)을 추출해야 한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image24.png alt=CS231n_lecture3_image4>
</p>
<p>이미지의 특징을 특징 벡터로 변환한다.</p>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image25.png alt=CS231n_lecture3_image25>
</p>
<p>왼쪽 그림에서 linear classifier로 결정계를 구분할 수 없기 때문에 Feature Transform(특징변환)을 하여 점들을 오른쪽 그림과 같이 선형 벡터로 변환한다. 오른쪽 그림에서 이제 선형함수로 분리할 수 있게 되었다.</p>
<p>그렇다면 어떤 방식으로 이미지를 feature transform 할 수 있을까?</p>
<ul>
<li>
<p><strong><strong>Color Histogram</strong></strong> : 특정 색에 해당하는 픽셀의 수를 계산하여 히스토그램으로 표현</p>
</li>
<li>
<p><strong><strong>HIstogram of Oriented Gradients (Hog)</strong></strong> : 이미지를 8x8 픽셀로 나누어 가장 지배적인 edge를 찾는다. Edge를 양자화시켜 edge orientation 히스토그램으로 표현한다.</p>
</li>
<li>
<p><strong><strong>Bag of Words</strong></strong> : 이미지를 작게 잘라 시각 단어로 표현한 후,K-means 알고리즘으로 1천개의 중심점을 갖도록 학습을 한다. 이미지의 시각 단어의 발생 빈도를 인코딩하여 이미지의 특징을 추출한다.</p>
</li>
</ul>
<p><img loading=lazy src=/images/CS231n/CS231n_lecture3_image26.png alt=CS231n_lecture3_image266>
</p>
<p>2012년까지는 Image feature을 추출하는 것이 중요했지만, AlexNet이 나온 뒤로는 layer을 쌓으면서 데이터로부터 특징을 직접 추출하여 학습하는 방식을 사용한다. Linear classifier, 가중치를 한 번에 학습하기 때문에 CNN의 구조를 잘 짜는 것이 더욱 중요해졌다.</p>
<hr>
<blockquote>
<p><em><strong><strong>참고</strong></strong></em></p>
<p><a href=https://jihyun22.github.io/dl/cs231n-03/>*[CS231n] Lecture 3 - Loss Suctions and Optimization*</a></p>
<p><a href=https://velog.io/@guide333/4.5-CS231n-3%EA%B0%951-Loss-Functions>*[풀잎스쿨] CS231n 3강(1) Loss Functions*</a></p>
<p><a href=https://velog.io/@guide333/%ED%92%80%EC%9E%8E%EC%8A%A4%EC%BF%A8-CS231n-3%EA%B0%95-2-Optimization>*[풀잎스쿨] CS231n 3강 (2) Optimization & Image Features*</a></p>
<p><a href="https://leechamin.tistory.com/85?category=830805">*[CS231n] 3. Loss Functions and Optimization*</a></p>
<p><a href=https://cding.tistory.com/2><em>누구나 이해할 수 있는 딥러닝 - cs231n 3강(Loss Functions and Optimization)</em></a></p>
</blockquote>
</div>
<footer class=post-footer>
<ul class=post-tags>
<li><a href=https://wecoding20.git.io/tags/cs231n/>CS231n</a></li>
<li><a href=https://wecoding20.git.io/tags/deep-learning/>deep-learning</a></li>
<li><a href=https://wecoding20.git.io/tags/computer-vision/>Computer-Vision</a></li>
</ul>
<nav class=paginav>
<a class=next href=https://wecoding20.git.io/post/cs231n_lecture1/>
<span class=title>Next Page »</span>
<br>
<span>[CS231n] Lecture 1: Introduction 강의노트</span>
</a>
</nav>
<div class=share-buttons>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on twitter" href="https://twitter.com/intent/tweet/?text=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8&url=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f&hashtags=CS231n%2cdeep-learning%2cComputer-Vision"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f&title=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8&summary=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8&source=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f&title=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on whatsapp" href="https://api.whatsapp.com/send?text=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8%20-%20https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg>
</a>
<a target=_blank rel="noopener noreferrer" aria-label="share [CS231n] Lecture 3: Loss Functions and Optimization 강의노트 on telegram" href="https://telegram.me/share/url?text=%5bCS231n%5d%20Lecture%203%3a%20Loss%20Functions%20and%20Optimization%20%ea%b0%95%ec%9d%98%eb%85%b8%ed%8a%b8&url=https%3a%2f%2fwecoding20.git.io%2fpost%2fcs231n_lecture3%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg>
</a>
</div>
</footer>
</article>
</main>
<footer class=footer>
<span>&copy; 2021 <a href=https://wecoding20.git.io/>잡다한 코딩스터디 WeCoding</a></span>
<span>
Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a>
</span>
</footer>
<a href=#top aria-label="go to top" title="Go to Top (Alt + G)">
<button class=top-link id=top-link type=button accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</button>
</a>
<script>let menu=document.getElementById('menu');menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)},document.querySelectorAll('a[href^="#"]').forEach(a=>{a.addEventListener("click",function(b){b.preventDefault();var a=this.getAttribute("href").substr(1);window.matchMedia('(prefers-reduced-motion: reduce)').matches?document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(a)}']`).scrollIntoView({behavior:"smooth"}),a==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${a}`)})})</script>
<script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script>
<script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove('dark'),localStorage.setItem("pref-theme",'light')):(document.body.classList.add('dark'),localStorage.setItem("pref-theme",'dark'))})</script>
</body>
</html>